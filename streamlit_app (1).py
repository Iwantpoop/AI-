# -*- coding: utf-8 -*-
"""streamlit_app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g6TaEd5g8V63aSo4GApD_vsPX6LCzyA8
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import yfinance as yf
# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import accuracy_score
# import requests # Import requests for web scraping
# 
# st.set_page_config(layout="wide")
# 
# st.title("ğŸ“ˆ AI Market Dashboard")
# 
# # -----------------------------------
# # âœ… ì§€ìˆ˜ êµ¬ì„± ì¢…ëª© ê°€ì ¸ì˜¤ê¸° (Wikipedia ìŠ¤í¬ë˜í•‘)
# # -----------------------------------
# @st.cache_data
# def get_sp500_constituents():
#     try:
#         headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
#         response = requests.get("https://en.wikipedia.org/wiki/List_of_S%26P_500_companies", headers=headers)
#         response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)
#         table = pd.read_html(response.text)[0]
#         return table["Symbol"].tolist()
#     except Exception as e:
#         st.error(f"S&P500 ì¢…ëª©ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}. ê¸°ë³¸ ì¢…ëª©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
#         return ["AAPL", "MSFT", "NVDA", "AMZN", "GOOGL"]
# 
# @st.cache_data
# def get_nasdaq100_constituents():
#     try:
#         headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
#         response = requests.get("https://en.wikipedia.org/wiki/NASDAQ-100", headers=headers)
#         response.raise_for_status()
#         tables = pd.read_html(response.text)
#         for table in tables:
#             if "Ticker" in table.columns:
#                 return table["Ticker"].tolist()
#         st.error("NASDAQ100 í…Œì´ë¸”ì—ì„œ 'Ticker' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì¢…ëª©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
#         return ["AAPL", "MSFT", "NVDA", "AMZN", "META", "TSLA"]
#     except Exception as e:
#         st.error(f"NASDAQ100 ì¢…ëª©ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}. ê¸°ë³¸ ì¢…ëª©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
#         return ["AAPL", "MSFT", "NVDA", "AMZN", "META", "TSLA"]
# 
# @st.cache_data
# def get_dow_constituents():
#     try:
#         headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
#         response = requests.get("https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average", headers=headers)
#         response.raise_for_status()
#         tables = pd.read_html(response.text)
#         for table in tables:
#             if "Symbol" in table.columns:
#                 return table["Symbol"].tolist()
#         st.error("DOW30 í…Œì´ë¸”ì—ì„œ 'Symbol' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì¢…ëª©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
#         return ["AAPL", "MSFT", "JPM", "V", "UNH", "HD", "PG"]
#     except Exception as e:
#         st.error(f"DOW30 ì¢…ëª©ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}. ê¸°ë³¸ ì¢…ëª©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
#         return ["AAPL", "MSFT", "JPM", "V", "UNH", "HD", "PG"]
# 
# # ----------------------------------
# # ğŸ”¹ ì‹œì¥ ì„ íƒ (ì‚¬ì´ë“œë°”)
# # ----------------------------------
# index_option = st.sidebar.selectbox(
#     "ì‹œì¥ ì„ íƒ",
#     ["S&P 500", "Nasdaq 100", "Dow Jones"]
# )
# 
# # ì„ íƒëœ ì‹œì¥ì— ë”°ë¥¸ ì „ì²´ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
# if index_option == "S&P 500":
#     all_market_tickers = get_sp500_constituents()
# elif index_option == "Nasdaq 100":
#     all_market_tickers = get_nasdaq100_constituents()
# else: # Dow Jones
#     all_market_tickers = get_dow_constituents()
# 
# # ----------------------------------
# # ğŸ”¹ ê°•ì„¸ ì¢…ëª© í•„í„°ë§ (MA20 > MA60)
# # ----------------------------------
# @st.cache_data
# def get_strong_tickers(market_tickers):
#     strong = []
#     # ì„±ëŠ¥ì„ ìœ„í•´ ì²˜ìŒ 50ê°œ ì¢…ëª©ë§Œ ë¶„ì„
#     for ticker in market_tickers[:50]:
#         try:
#             data = yf.download(ticker, period="6mo", progress=False)
#             if len(data) < 60: # MA ê³„ì‚°ì— ì¶©ë¶„í•œ ë°ì´í„° í™•ì¸
#                 continue
#             data["MA20"] = data["Close"].rolling(20).mean()
#             data["MA60"] = data["Close"].rolling(60).mean()
#             if data["MA20"].iloc[-1] > data["MA60"].iloc[-1]:
#                 strong.append(ticker)
#         except:
#             continue
#     return strong
# 
# strong_tickers_list = get_strong_tickers(all_market_tickers)
# 
# # ê°•ì„¸ ì¢…ëª©ì´ ì ì„ ê²½ìš° í´ë°±
# if not strong_tickers_list or len(strong_tickers_list) < 5:
#     st.sidebar.info("ê°•ì„¸ ì¢…ëª© í•„í„°ë§ ê²°ê³¼ê°€ ì ì–´, ë‹¤ë¥¸ ì§€ìˆ˜ ë‚´ ì¢…ëª©ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤.")
#     temp_list = list(strong_tickers_list)
#     added_count = 0
#     for t in all_market_tickers:
#         if t not in temp_list and added_count < 10: # ìµœëŒ€ 10ê°œ ì¶”ê°€ ì¢…ëª©
#             temp_list.append(t)
#             added_count += 1
#     if not temp_list: # ìµœì¢… í´ë°± (ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ)
#         temp_list = ["AAPL", "MSFT", "GOOGL", "AMZN", "TSLA", "NVDA", "JPM"]
#     strong_tickers_list = temp_list
# 
# # ----------------------------------
# # ğŸ”¹ ê°•ì„¸ ì¢…ëª© ì„ íƒ (ì‚¬ì´ë“œë°”)
# # ----------------------------------
# selected_stock_ticker = st.sidebar.selectbox("ê°•ì„¸ ì¢…ëª© ì„ íƒ", strong_tickers_list)
# 
# # ----------------------------------
# # ğŸ”¹ ë°ì´í„° ë¡œë“œ (ì„ íƒëœ ì¢…ëª©)
# # ----------------------------------
# data = yf.download(selected_stock_ticker, period="2y", interval="1d")
# data.dropna(inplace=True)
# 
# # ----------------------------------
# # ğŸ”¹ ì§€í‘œ ìƒì„± (ì„ íƒëœ ì¢…ëª©)
# # ----------------------------------
# data["Return"] = data["Close"].pct_change()
# data["MA20"] = data["Close"].rolling(20).mean()
# data["MA60"] = data["Close"].rolling(60).mean()
# data["Volatility"] = data["Return"].rolling(20).std()
# 
# data.dropna(inplace=True)
# 
# # ----------------------------------
# # ğŸ”¹ ë¨¸ì‹ ëŸ¬ë‹ ë¶„ë¥˜ (ë‹¤ìŒë‚  ìƒìŠ¹ ì˜ˆì¸¡ - ì„ íƒëœ ì¢…ëª©)
# # ----------------------------------
# data["Target"] = np.where(data["Return"].shift(-1) > 0, 1, 0)
# 
# features = ["MA20", "MA60", "Volatility"]
# X = data[features]
# y = data["Target"]
# 
# X_train, X_test, y_train, y_test = train_test_split(
#     X, y, shuffle=False, test_size=0.2
# )
# 
# model = RandomForestClassifier()
# model.fit(X_train, y_train)
# 
# preds = model.predict(X_test)
# accuracy = accuracy_score(y_test, preds)
# 
# # ì˜¤ëŠ˜ ìƒìŠ¹ í™•ë¥ 
# latest_features = X.iloc[-1:].values
# up_prob = model.predict_proba(latest_features)[0][1]
# 
# # ----------------------------------
# # ğŸ”¹ ì „ëµ ìˆ˜ìµë¥  ê³„ì‚° (ì„ íƒëœ ì¢…ëª©)
# # ----------------------------------
# data["Prediction"] = model.predict(X)
# data["Strategy_Return"] = data["Prediction"].shift(1) * data["Return"]
# 
# strategy_cum = (1 + data["Strategy_Return"]).cumprod()
# market_cum = (1 + data["Return"]).cumprod()
# 
# strategy_return = (strategy_cum.iloc[-1] - 1) * 100
# market_return = (market_cum.iloc[-1] - 1) * 100
# 
# current_price = float(data["Close"].iloc[-1]) # Ensure current_price is a scalar float
# 
# # ----------------------------------
# # ğŸ”® 30ì¼ ëª¬í…Œì¹´ë¥¼ë¡œ ì˜ˆì¸¡ (ì„ íƒëœ ì¢…ëª©)
# # ----------------------------------
# future_days = 30
# returns = data["Return"].dropna()
# mu = returns.tail(60).mean()
# sigma = returns.tail(60).std()
# 
# last_price = current_price
# 
# simulations = 200
# price_paths = []
# 
# for _ in range(simulations):
#     prices = [last_price]
#     for _ in range(future_days):
#         shock = np.random.normal(mu, sigma)
#         prices.append(prices[-1] * (1 + shock))
#     price_paths.append(prices)
# 
# price_paths = np.array(price_paths)
# mean_path = price_paths.mean(axis=0)
# 
# # Ensure last_price is a scalar float to prevent pandas type alignment issues
# expected_returns = (mean_path - float(last_price)) / float(last_price)
# 
# ideal_day = np.argmax(expected_returns)
# ideal_price = mean_path[ideal_day]
# ideal_return = expected_returns[ideal_day] * 100
# 
# # ----------------------------------
# # ğŸ“Š ì§€í‘œ UI í‘œì‹œ (ì„ íƒëœ ì¢…ëª©)
# # ----------------------------------
# st.subheader(f"ğŸ“Š {selected_stock_ticker} í•µì‹¬ ì§€í‘œ")
# 
# col1, col2, col3 = st.columns(3)
# col4, col5, col6 = st.columns(3)
# 
# col1.metric("í˜„ì¬ê°€", f"{current_price:.2f}")
# col2.metric("ìƒìŠ¹ í™•ë¥ ", f"{up_prob*100:.2f}%")
# col3.metric("ëª¨ë¸ ì •í™•ë„", f"{accuracy*100:.2f}%")
# 
# col4.metric("ì „ëµ ìˆ˜ìµë¥  (2ë…„)", f"{strategy_return:.2f}%")
# col5.metric("ì‹œì¥ ìˆ˜ìµë¥  (2ë…„)", f"{market_return:.2f}%")
# col6.metric("ì´ìƒì  ë³´ìœ ê¸°ê°„", f"{ideal_day}ì¼")
# 
# st.markdown(
#     f"ğŸ“Œ ë³´ìœ ê¸°ê°„ í›„ ì˜ˆìƒ ì£¼ê°€: **{ideal_price:.2f}**  \n"
#     f"ğŸ“ˆ ê¸°ëŒ€ ìˆ˜ìµë¥ : **{ideal_return:.2f}%**"
# )
# 
# # ----------------------------------
# # ğŸ“ˆ ì°¨íŠ¸ (ì„ íƒëœ ì¢…ëª©)
# # ----------------------------------
# st.subheader(f"ğŸ“ˆ {selected_stock_ticker} ê°€ê²© & ì˜ˆì¸¡")
# 
# fig = plt.figure(figsize=(12,6))
# plt.plot(data.index, data["Close"], label="Historical")
# 
# future_dates = pd.date_range(
#     data.index[-1], periods=future_days+1, freq="B"
# )
# 
# plt.plot(future_dates, mean_path, label="Predicted")
# 
# plt.scatter(future_dates[ideal_day], ideal_price, s=100)
# 
# plt.legend()
# st.pyplot(fig)

from pyngrok import ngrok
import subprocess
import os

# Kill any existing ngrok tunnels
os.system("kill -9 $(lsof -t -i:8501)")
ngrok.kill()

# Set up ngrok tunnel
public_url = ngrok.connect(8501)
print(f"Streamlit app available at: {public_url}")

# Run Streamlit in the background as a Python module
subprocess.Popen(["python", "-m", "streamlit", "run", "app.py", "--server.port", "8501", "--server.headless", "true"])

!pip install pyngrok

pip install streamlit yfinance pandas numpy ta requests scikit-learn xgboost

from pyngrok import ngrok

# Replace 'YOUR_NGROK_AUTH_TOKEN' with your actual ngrok authtoken
ngrok.set_auth_token("39bE0CpS2WHAaUX6PQ6rASAYJ0O_31kmVDWd725qaQTHa83Sz")